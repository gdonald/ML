{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86b8ccd",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9f357",
   "metadata": {},
   "source": [
    "#### Extract tar file\n",
    "\n",
    "Check whether the aclImdb directory exists. If it does not, open aclImdb_v1.tar.gz and extract its contents into the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd8cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb already exists. Skipping aclImdb_v1.tar.gz extraction.\n"
     ]
    }
   ],
   "source": [
    "import os, tarfile\n",
    "\n",
    "tar_path = \"aclImdb_v1.tar.gz\"\n",
    "data_path = \"aclImdb\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\".\", filter=\"data\")\n",
    "    print(f\"{tar_path} extraction complete.\")\n",
    "else:\n",
    "    print(f\"{data_path} already exists. Skipping {tar_path} extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6a4b0",
   "metadata": {},
   "source": [
    "#### Try to setup local Apple device\n",
    "\n",
    "Select a compute device (I am using a Macbook Pro M4). Prefer Apple MPS if available and set device to \"mps\". Otherwise fall back to \"cpu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da94c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5a706",
   "metadata": {},
   "source": [
    "#### Split training data\n",
    "\n",
    "Set the dataset root from data_path. Build paths to the train and test positive and negative folders. Define a loader that reads every .txt review. Cleans <br /> tags, strip whitespace, and assign labels. Read and combine the positive and negative training reviews with labels 1 and 0 and then create a DataFrame. Create a stratified 90 to 10 train and validation split with a fixed random seed. Read and combine the positive and negative test reviews into a separate DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679f9b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df): 22500\n",
      "len(val_df): 2500\n",
      "len(test_df): 25000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "root = Path(data_path)\n",
    "train_pos = root / \"train\" / \"pos\"\n",
    "train_neg = root / \"train\" / \"neg\"\n",
    "test_pos = root / \"test\" / \"pos\"\n",
    "test_neg = root / \"test\" / \"neg\"\n",
    "\n",
    "\n",
    "def load_dir(d: Path, label: int):\n",
    "    rows = []\n",
    "    for fp in d.glob(\"*.txt\"):\n",
    "        txt = fp.read_text(encoding=\"utf-8\")\n",
    "        rows.append({\"text\": txt.replace(\"<br />\", \"\\n\").strip(), \"label\": label})\n",
    "    return rows\n",
    "\n",
    "\n",
    "train_rows = load_dir(train_pos, 1) + load_dir(train_neg, 0)\n",
    "df = pd.DataFrame(train_rows)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.1, random_state=17, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "test_rows = load_dir(test_pos, 1) + load_dir(test_neg, 0)\n",
    "test_df = pd.DataFrame(test_rows)\n",
    "\n",
    "print(f\"len(train_df): {len(train_df)}\")\n",
    "print(f\"len(val_df): {len(val_df)}\")\n",
    "print(f\"len(test_df): {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988dba5",
   "metadata": {},
   "source": [
    "#### Perform tokenization\n",
    "\n",
    "Convert the train, validation, and test DataFrames into Hugging Face Dataset objects and then combine them into a DatasetDict. Load the DistilBERT tokenizer. Define a batch tokenization function that truncates and pads to length 256. Map it across all splits while removing the text column. Rename label to labels to match the model’s expected input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f4c2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 22500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7345dca7ad564b73acbd46aff0c07455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b223df1c40e748a69048eab55e03c53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba782e8cd304ca4a47ca7ee428ccc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 22500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(train_ds)\n",
    "print(val_ds)\n",
    "print(test_ds)\n",
    "\n",
    "raw = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_ds,\n",
    "        \"validation\": val_ds,\n",
    "        \"test\": test_ds,\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized = raw.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(tokenized)\n",
    "print(tokenized[\"train\"][0].keys())\n",
    "print(len(tokenized[\"train\"][0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8951ef",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "\n",
    "Load DistilBERT sequence classification model configured for two labels. Define a helper function that counts total parameters and the subset that are trainable. Compute counts for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dcdd19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params total: 66955010  trainable: 66955010\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters()), sum(\n",
    "        p.numel() for p in m.parameters() if p.requires_grad\n",
    "    )\n",
    "\n",
    "\n",
    "total, trainable = count_params(model)\n",
    "\n",
    "print(f\"Params total: {total}  trainable: {trainable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084bb392",
   "metadata": {},
   "source": [
    "#### Train for several epochs\n",
    "\n",
    "Load a new DistilBERT sequence classifier with two labels. Define a metric function that converts logits to predictions and computes accuracy and F1. Create training arguments that set output directory, batch sizes, learning rate, epoch count, weight decay, logging frequency, and seed. Build a Trainer with the model, arguments, tokenized training data, validation data, and metrics function. Start training for three epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7533306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8439' max='8439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8439/8439 12:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8439, training_loss=0.20985117692039934, metrics={'train_runtime': 783.5032, 'train_samples_per_second': 86.152, 'train_steps_per_second': 10.771, 'total_flos': 4470774704640000.0, 'train_loss': 0.20985117692039934, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"imdb-distilbert\",\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=17,\n",
    "    logging_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694eb6c",
   "metadata": {},
   "source": [
    "#### Evaluate training results\n",
    "\n",
    "Evaluate the trained model on the tokenized test set. Capture the resulting metrics dictionary and inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12262216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"eval_loss\": 0.3970693349838257,\n",
      "  \"eval_accuracy\": 0.91668,\n",
      "  \"eval_precision\": 0.9167116860898716,\n",
      "  \"eval_recall\": 0.9166799999999999,\n",
      "  \"eval_f1\": 0.9166784160900185,\n",
      "  \"eval_runtime\": 88.6076,\n",
      "  \"eval_samples_per_second\": 282.143,\n",
      "  \"eval_steps_per_second\": 17.64,\n",
      "  \"epoch\": 3.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "test_metrics = trainer.evaluate(tokenized[\"test\"])\n",
    "\n",
    "print(json.dumps(test_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fb09f",
   "metadata": {},
   "source": [
    "#### Save the model\n",
    "\n",
    "Save the trained model and config. Load the DistilBERT tokenizer checkpoint and write out its tokenizer files. Ensure both model and tokenizer can be reloaded together for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345ced12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('imdb-distilbert/best/tokenizer_config.json',\n",
       " 'imdb-distilbert/best/special_tokens_map.json',\n",
       " 'imdb-distilbert/best/vocab.txt',\n",
       " 'imdb-distilbert/best/added_tokens.json',\n",
       " 'imdb-distilbert/best/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "save_dir = \"imdb-distilbert/best\"\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "AutoTokenizer.from_pretrained(\"distilbert-base-uncased\").save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafc930",
   "metadata": {},
   "source": [
    "#### Fix labels\n",
    "\n",
    "Load the saved classifier. Assign readable label mappings for both id2label and label2id. Save the updated model so future loads return the human readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6d2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"imdb-distilbert/best\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir).to(device)\n",
    "model.config.id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "model.config.label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2cf93",
   "metadata": {},
   "source": [
    "#### Test sample reviews\n",
    "\n",
    "Create a sentiment analysis pipeline using the saved model. Run the pipeline on two texts to produce predicted sentiment labels and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767c0e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9988242983818054},\n",
       " {'label': 'NEGATIVE', 'score': 0.9988081455230713}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"sentiment-analysis\", model=\"imdb-distilbert/best\")\n",
    "\n",
    "texts = [\n",
    "    \"I thought the movie was outstanding. The acting pulled me in.\",\n",
    "    \"I thought the movie was terrible. The acting was dull and uninspired.\",\n",
    "]\n",
    "\n",
    "clf(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193a437",
   "metadata": {},
   "source": [
    "#### Inspect performance\n",
    "\n",
    "Generate predictions on the tokenized test set. Extract true labels and derive predicted class indices with argmax over the logits. Compute a confusion matrix from the labels. Build a ConfusionMatrixDisplay and render the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3850d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.91      0.92      0.92     12500\n",
      "    POSITIVE       0.92      0.91      0.92     12500\n",
      "\n",
      "    accuracy                           0.92     25000\n",
      "   macro avg       0.92      0.92      0.92     25000\n",
      "weighted avg       0.92      0.92      0.92     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "pred = trainer.predict(tokenized[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"NEGATIVE\", \"POSITIVE\"]))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"NEGATIVE\", \"POSITIVE\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, values_format=\"d\")\n",
    "ax.set_title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8880c10",
   "metadata": {},
   "source": [
    "#### Find mistakes\n",
    "\n",
    "Generate predictions on the test set and convert logits to probabilities with softmax. Extract true labels and predicted classes. Identify misclassified indices and sort them by model confidence in descending order. Select the top three false positives and the top three false negatives. Define a helper that prints each selected sample’s index, true and predicted labels, the positive and negative probabilities, and a truncated snippet of the review text. Call the helper to display the false positives and then the false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e89bbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives\n",
      "idx=23766 true=0 pred=1 p_pos=0.999 p_neg=0.001\n",
      "There are those who gripe that this is NOT the opera, but then they don't quibble with the film of CABARET that was not the original show either. All films of musicals/operas are and have to be \"adaptations\" or they...\n",
      "\n",
      "idx=19838 true=0 pred=1 p_pos=0.999 p_neg=0.001\n",
      "This is definitely one of the best Kung fu movies in the history of Cinema. The screenplay is really well done (which is not often the case for this type of movies) and you can see that Chuck (in one of his first...\n",
      "\n",
      "idx=23777 true=0 pred=1 p_pos=0.999 p_neg=0.001\n",
      "This has to be one of the all time greatest horror movies. Charles Band made the best movie of 96' in this little seen gem. Highly realistic and , incredibly stylised- with a visual flair David Fincher would envy, its...\n",
      "\n",
      "False negatives\n",
      "idx=5241 true=1 pred=0 p_pos=0.001 p_neg=0.999\n",
      "Stupidly beautiful. This movie epitomizes the 'so bad it's good' genre of films. The only two talents in it are Richard Boone and Joan van Ark, and only Boone is any good. It's kind of sad that the man who rose to...\n",
      "\n",
      "idx=5927 true=1 pred=0 p_pos=0.001 p_neg=0.999\n",
      "I just saw this movie in a sneak preview and before reading my comment you have to know that it is very subjective because I love Techno, Trance, Club, House and music like that. The movie deals with Carl, whose...\n",
      "\n",
      "idx=10051 true=1 pred=0 p_pos=0.001 p_neg=0.999\n",
      "The film had NO help at all, promotion-wise: if there was an advertising promo on TV or radio, I didn't see/hear it. The only newspaper ad I saw was on it's opening weekend: a dingy, sludgy B & W head-shot photo of...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.nn.functional import softmax\n",
    "from textwrap import shorten\n",
    "\n",
    "pred = trainer.predict(tokenized[\"test\"])\n",
    "logits = torch.tensor(pred.predictions)\n",
    "probs = softmax(logits, dim=-1).numpy()\n",
    "y_true = pred.label_ids\n",
    "y_pred = probs.argmax(axis=1)\n",
    "\n",
    "wrong = np.where(y_pred != y_true)[0]\n",
    "conf_wrong = wrong[np.argsort(probs[wrong].max(axis=1))[::-1]]\n",
    "\n",
    "fp = [i for i in conf_wrong if y_pred[i] == 1][:3]\n",
    "fn = [i for i in conf_wrong if y_pred[i] == 0][:3]\n",
    "\n",
    "\n",
    "def show(ixs, title):\n",
    "    print(title)\n",
    "    for i in ixs:\n",
    "        print(\n",
    "            f\"idx={i} true={y_true[i]} pred={y_pred[i]} p_pos={probs[i,1]:.3f} p_neg={probs[i,0]:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            shorten(\n",
    "                test_df.iloc[i][\"text\"].replace(\"\\n\", \" \"), width=220, placeholder=\"...\"\n",
    "            )\n",
    "        )\n",
    "        print()\n",
    "\n",
    "\n",
    "show(fp, \"False positives\")\n",
    "show(fn, \"False negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff96bc9",
   "metadata": {},
   "source": [
    "#### Verify index alignment\n",
    "\n",
    "Run prediction on the test split. Extract true labels and compute predicted classes, then find the indices where predictions differ from truth. For the first five misclassifications, compare the label stored in the tokenized dataset to the label in the original DataFrame to verify alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc1fc693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 17 | dataset_label: 1 | df_label: 1\n",
      "idx: 55 | dataset_label: 1 | df_label: 1\n",
      "idx: 58 | dataset_label: 1 | df_label: 1\n",
      "idx: 62 | dataset_label: 1 | df_label: 1\n",
      "idx: 70 | dataset_label: 1 | df_label: 1\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(tokenized[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = pred.predictions.argmax(axis=1)\n",
    "wrong = np.where(y_pred != y_true)[0]\n",
    "\n",
    "for i in wrong[:5]:\n",
    "    print(\n",
    "        \"idx:\",\n",
    "        i,\n",
    "        \"| dataset_label:\",\n",
    "        int(tokenized[\"test\"][i][\"labels\"]),\n",
    "        \"| df_label:\",\n",
    "        int(test_df.iloc[i][\"label\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040507b",
   "metadata": {},
   "source": [
    "#### Check truncation\n",
    "\n",
    "Load the DistilBERT tokenizer. Randomly sample 2K training reviews. Tokenize each sample without truncation and measure token sequence lengths. Compute the fraction of samples longer than 256 tokens and the maximum observed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522af1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction > 256: 0.41 Max length: 1799\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.model_max_length = 10_000\n",
    "\n",
    "sample = train_df[\"text\"].sample(2000, random_state=17).tolist()\n",
    "lens = [len(tokenizer(t, truncation=False)[\"input_ids\"]) for t in sample]\n",
    "\n",
    "frac_trunc = float(np.mean(np.array(lens) > 256))\n",
    "max_len = int(np.max(lens))\n",
    "\n",
    "print(\"Fraction > 256:\", round(frac_trunc, 3), \"Max length:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3392f05",
   "metadata": {},
   "source": [
    "#### Re-tokenize at longer maximum length\n",
    "\n",
    "Create a new DatasetDict from the train, validation, and test splits. Load the DistilBERT tokenizer. Define a tokenization function that truncates and pads each review to a longer length of 320. Map this function across all splits in batch. Drop the text column, and rename label to labels to match the model input. Verify that token sequences are length 320."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea8dcb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e9ce42225c4b45bf53a681e4c73e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7cbf0431624f599c5c3a91f2eb4622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723876cc8b4446c78954bf34c9670b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids: 320\n"
     ]
    }
   ],
   "source": [
    "raw = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_ds,\n",
    "        \"validation\": val_ds,\n",
    "        \"test\": test_ds,\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tok320(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], truncation=True, padding=\"max_length\", max_length=320\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_320 = raw.map(tok320, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_320 = tokenized_320.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(f\"Length of input_ids: {len(tokenized_320['train'][0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1162d32",
   "metadata": {},
   "source": [
    "#### Retrain for one epoch\n",
    "\n",
    "Load a DistilBERT classifier. Set training hyperparameters for a one-epoch run. Construct a Trainer with the model with the 320-tokenized train and validation splits. Train the model for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85f58233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2813' max='2813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2813/2813 05:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2813, training_loss=0.2955135449878355, metrics={'train_runtime': 322.9919, 'train_samples_per_second': 69.661, 'train_steps_per_second': 8.709, 'total_flos': 1862822793600000.0, 'train_loss': 0.2955135449878355, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_320 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "args_320 = TrainingArguments(\n",
    "    output_dir=\"imdb-distilbert-320\",\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=17,\n",
    "    logging_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer_320 = Trainer(\n",
    "    model=model_320,\n",
    "    args=args_320,\n",
    "    train_dataset=tokenized_320[\"train\"],\n",
    "    eval_dataset=tokenized_320[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer_320.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2e9a4",
   "metadata": {},
   "source": [
    "#### Evaluate new model\n",
    "\n",
    "Evaluate the model on the 320-token test split using the trainer and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d8523ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"eval_loss\": 0.25842297077178955,\n",
      "  \"eval_runtime\": 104.6173,\n",
      "  \"eval_samples_per_second\": 238.966,\n",
      "  \"eval_steps_per_second\": 14.94,\n",
      "  \"epoch\": 1.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_metrics_320 = trainer_320.evaluate(tokenized_320[\"test\"])\n",
    "\n",
    "print(json.dumps(test_metrics_320, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425c54a",
   "metadata": {},
   "source": [
    "#### Compute accuracy, precision, recall, and F1\n",
    "\n",
    "Run predictions on the 320 token test split. Extract true labels and derive predicted classes by taking argmax over the logits. Compute accuracy, precision, recall, and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d6e8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9189\n",
      "precision: 0.9138\n",
      "recall: 0.9251\n",
      "f1: 0.9194\n"
     ]
    }
   ],
   "source": [
    "pred = trainer_320.predict(tokenized_320[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = pred.predictions.argmax(axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "print(f\"precision: {precision:.4f}\")\n",
    "print(f\"recall: {recall:.4f}\")\n",
    "print(f\"f1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba31699",
   "metadata": {},
   "source": [
    "#### Retrain for three epochs\n",
    "\n",
    "Load a new DistilBERT classifier. Configure training for three epochs and write out the outputs. Build a Trainer using the 320-tokenized train and validation splits. Start training for three epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "879607e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8439' max='8439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8439/8439 16:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8439, training_loss=0.1959439560281876, metrics={'train_runtime': 995.7616, 'train_samples_per_second': 67.787, 'train_steps_per_second': 8.475, 'total_flos': 5588468380800000.0, 'train_loss': 0.1959439560281876, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_320e3 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "args_320e3 = TrainingArguments(\n",
    "    output_dir=\"imdb-distilbert-320e3\",\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=17,\n",
    "    logging_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer_320e3 = Trainer(\n",
    "    model=model_320e3,\n",
    "    args=args_320e3,\n",
    "    train_dataset=tokenized_320[\"train\"],\n",
    "    eval_dataset=tokenized_320[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer_320e3.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adedcd73",
   "metadata": {},
   "source": [
    "#### Test again\n",
    "\n",
    "Evaluate the new three epoch model on the 320 token test split and store the resulting metrics dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9b00f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"eval_loss\": 0.3706631362438202,\n",
      "  \"eval_runtime\": 104.4151,\n",
      "  \"eval_samples_per_second\": 239.429,\n",
      "  \"eval_steps_per_second\": 14.969,\n",
      "  \"epoch\": 3.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_metrics_320e3 = trainer_320e3.evaluate(tokenized_320[\"test\"])\n",
    "\n",
    "print(json.dumps(test_metrics_320e3, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bc4bc",
   "metadata": {},
   "source": [
    "#### Compute accuracy, precision, recall, and F1 again\n",
    "\n",
    "Run predictions on the 320e3 test split. Extract true labels and convert logits to class indices. Compute accuracy, precision, recall, and F1 from those arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dca7efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9220\n",
      "precision: 0.9233\n",
      "recall: 0.9205\n",
      "f1: 0.9219\n"
     ]
    }
   ],
   "source": [
    "pred = trainer_320e3.predict(tokenized_320[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = pred.predictions.argmax(axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "print(f\"precision: {precision:.4f}\")\n",
    "print(f\"recall: {recall:.4f}\")\n",
    "print(f\"f1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344352b",
   "metadata": {},
   "source": [
    "#### Save model and fix labels again\n",
    "\n",
    "Save the trained model, config, and tokenizer files. Reload the model from disk and assign human readable label maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd74ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"imdb-distilbert-320e3/best\"\n",
    "\n",
    "trainer_320e3.save_model(save_dir)\n",
    "AutoTokenizer.from_pretrained(\"distilbert-base-uncased\").save_pretrained(save_dir)\n",
    "\n",
    "m = AutoModelForSequenceClassification.from_pretrained(save_dir).to(device)\n",
    "m.config.id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "m.config.label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "m.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e84d52",
   "metadata": {},
   "source": [
    "#### Test latest model\n",
    "\n",
    "Create a sentiment analysis pipeline using the saved best model. Define three review texts and run the pipeline on them to generate predictions. Extract each prediction’s label and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c295b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis results:\n",
      "{('POSITIVE', 0.8788), ('NEGATIVE', 0.9985), ('POSITIVE', 0.9989)}\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=\"imdb-distilbert-320e3/best\")\n",
    "\n",
    "texts = [\n",
    "    \"Loved it. Performances were excellent.\",\n",
    "    \"Hated it. Plot was a mess.\",\n",
    "    \"Mixed feelings. Some scenes worked, others dragged.\",\n",
    "]\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print({(d['label'], round(d['score'], 4)) for d in clf(texts)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8176525",
   "metadata": {},
   "source": [
    "#### Add predict helper function\n",
    "\n",
    "Load the tokenizer and the new classifier. Move the model to the device and switch to evaluation mode. Read the label mapping from the model config. Define a \"predict\" helper function to accept a string or list. Tokenize with padding and truncation to a maximum length of 320 and move tensors to the device. Run the model without gradients. Apply softmax to logits, take the highest probability, and its class index. Map the index to a label, and return (label, confidence) pairs. Call predict on three example texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf025f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('POSITIVE', 0.9989264607429504),\n",
       " ('NEGATIVE', 0.9984642267227173),\n",
       " ('POSITIVE', 0.8788363337516785)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"imdb-distilbert-320e3/best\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(save_dir)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(save_dir).to(device).eval()\n",
    "id2label = mdl.config.id2label\n",
    "\n",
    "\n",
    "def predict(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    enc = tok(\n",
    "        texts, padding=True, truncation=True, max_length=320, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = mdl(**enc).logits\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        conf, pred = probs.max(dim=1)\n",
    "    return [(id2label[int(p)], float(c)) for p, c in zip(pred, conf)]\n",
    "\n",
    "\n",
    "predict(\n",
    "    [\n",
    "        \"Loved it. Performances were excellent.\",\n",
    "        \"Hated it. Plot was a mess.\",\n",
    "        \"Mixed feelings. Some scenes worked, others dragged.\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148bfdbb",
   "metadata": {},
   "source": [
    "#### Tune the decision threshold\n",
    "\n",
    "Run the model and convert logits to positive class probabilities. Sweep decision thresholds from 0.10 to 0.90 to find the threshold that maximizes F1 on validation. Use the chosen threshold to convert the test probabilities to binary. Compute test accuracy and F1 at that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76395016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_t: 0.88 val_acc: 0.9164 val_f1: 0.9147\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9219\n",
      "precision: 0.9329\n",
      "recall: 0.9091\n",
      "f1: 0.9209\n"
     ]
    }
   ],
   "source": [
    "pred_val = trainer_320e3.predict(tokenized_320[\"validation\"])\n",
    "p_pos_val = softmax(torch.tensor(pred_val.predictions), dim=-1).numpy()[:, 1]\n",
    "y_val = pred_val.label_ids\n",
    "\n",
    "ths = np.linspace(0.10, 0.90, 81)\n",
    "best_t, best_acc, best_f1 = 0.5, 0.0, 0.0\n",
    "\n",
    "for t in ths:\n",
    "    yhat = (p_pos_val >= t).astype(int)\n",
    "    acc = accuracy_score(y_val, yhat)\n",
    "    f1 = f1_score(y_val, yhat)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_t, best_acc, best_f1 = t, acc, f1\n",
    "\n",
    "print(\n",
    "    \"best_t:\",\n",
    "    round(best_t, 3),\n",
    "    \"val_acc:\",\n",
    "    round(best_acc, 4),\n",
    "    \"val_f1:\",\n",
    "    round(best_f1, 4),\n",
    ")\n",
    "\n",
    "pred_test = trainer_320e3.predict(tokenized_320[\"test\"])\n",
    "p_pos_test = softmax(torch.tensor(pred_test.predictions), dim=-1).numpy()[:, 1]\n",
    "y_test = pred_test.label_ids\n",
    "\n",
    "accuracy = accuracy_score(y_test, (p_pos_test >= best_t).astype(int))\n",
    "precision = precision_score(y_test, (p_pos_test >= best_t).astype(int))\n",
    "recall = recall_score(y_test, (p_pos_test >= best_t).astype(int))\n",
    "f1 = f1_score(y_test, (p_pos_test >= best_t).astype(int))\n",
    "\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "print(f\"precision: {precision:.4f}\")\n",
    "print(f\"recall: {recall:.4f}\")\n",
    "print(f\"f1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06286bb2",
   "metadata": {},
   "source": [
    "#### Save calculated threshold\n",
    "\n",
    "Save the current best_t value as JSON to persist the tuned decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c44eb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"imdb-threshold.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": float(best_t)}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a8c5f",
   "metadata": {},
   "source": [
    "#### Use threshold\n",
    "\n",
    "Load the tokenizer and the saved classifier. Read the tuned decision threshold from the json. Define a predict_threshold helper function to run the model to get positive class probabilities. Call the function on three example reviews and store the returned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e05fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted result:\n",
      "[('POSITIVE', 0.9982012510299683), ('NEGATIVE', 0.0074090054258704185), ('POSITIVE', 0.9955457448959351)]\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"imdb-distilbert-320e3/best\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(save_dir)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(save_dir).to(device).eval()\n",
    "\n",
    "t = json.load(open(\"imdb-threshold.json\"))[\"threshold\"]\n",
    "id2label = mdl.config.id2label\n",
    "\n",
    "\n",
    "def predict_threshold(texts, max_length=320, thresh=t):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    enc = tok(\n",
    "        texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    enc = { k: v.to(device) for k, v in enc.items() }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = softmax(mdl(**enc).logits, dim=-1)[:, 1]\n",
    "\n",
    "    labels = (probs >= thresh).long()\n",
    "\n",
    "    return [(id2label[int(l)], float(p)) for l, p in zip(labels, probs)]\n",
    "\n",
    "\n",
    "result = predict_threshold([\"Loved it\", \"Hated it\", \"Mixed but mostly fine\"])\n",
    "\n",
    "print(\"Predicted result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895022d",
   "metadata": {},
   "source": [
    "#### Implement early stopping\n",
    "\n",
    "Load a new DistilBERT classifier. Configure one epoch per loop. Build a Trainer on the 320-token datasets. Train for up to ten epochs one at a time and evaluate after each one. Track the best validation loss with a small tolerance. Save the model whenever validation loss improves. Stop early when validation loss fails to improve for two consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16b7a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2813' max='2813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2813/2813 05:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 11:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 val_loss 0.2931\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2813' max='2813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2813/2813 05:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 val_loss 0.3030\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2813' max='2813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2813/2813 05:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 val_loss 0.3257\n",
      "Early stopped\n"
     ]
    }
   ],
   "source": [
    "model_es = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "args_es = TrainingArguments(\n",
    "    output_dir=\"imdb-distilbert-es\",\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=17,\n",
    "    logging_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer_es = Trainer(\n",
    "    model=model_es,\n",
    "    args=args_es,\n",
    "    train_dataset=tokenized_320[\"train\"],\n",
    "    eval_dataset=tokenized_320[\"validation\"],\n",
    ")\n",
    "\n",
    "best = float(\"inf\")\n",
    "patience = 2\n",
    "stale = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    trainer_es.train()\n",
    "    metrics = trainer_es.evaluate()\n",
    "    val_loss = metrics[\"eval_loss\"]\n",
    "\n",
    "    print(f\"epoch {epoch + 1} val_loss {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss + 1e-5 < best:\n",
    "        best = val_loss\n",
    "        stale = 0\n",
    "\n",
    "        trainer_es.save_model(\"imdb-distilbert-es/best\")\n",
    "    else:\n",
    "        stale += 1\n",
    "        \n",
    "        if stale >= patience:\n",
    "            print(\"Early stopped\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37c77b",
   "metadata": {},
   "source": [
    "#### Evaluate on the best early stopping checkpoint\n",
    "\n",
    "Load the saved classifier. Build a Trainer using the model and the 320-tokenized test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62b136f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 05:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics: {'eval_loss': 0.2590813636779785, 'eval_model_preparation_time': 0.0005, 'eval_runtime': 304.038, 'eval_samples_per_second': 82.227, 'eval_steps_per_second': 2.572}\n"
     ]
    }
   ],
   "source": [
    "best_path = \"imdb-distilbert-es/best\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_path).to(device).eval()\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"imdb-distilbert-es/eval-cpu\",\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_eval_batch_size=32,\n",
    "    use_cpu=True,\n",
    "    use_mps_device=False,\n",
    "    seed=17,\n",
    "    logging_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    eval_dataset=tokenized_320[\"test\"],\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"metrics: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
