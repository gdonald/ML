{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4972de65",
   "metadata": {},
   "source": [
    "# Mastodon Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20601cf",
   "metadata": {},
   "source": [
    "#### Install requirements\n",
    "\n",
    "Uncomment to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c293049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d6f2d",
   "metadata": {},
   "source": [
    "#### Load access token\n",
    "\n",
    "Access token is stored in .env file.  A new token is generated at https://mastodon.social/settings/applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730f6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "ACCESS_TOKEN = os.environ[\"ACCESS_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe34312",
   "metadata": {},
   "source": [
    "#### Get data\n",
    "\n",
    "Run the data acquisition pipeline for Mastodon posts and persist a local CSV file. On subsequent runs, check whether the CSV file exists and has more than 500 lines, then load it to avoid refetching. Otherwise connect to mastodon.social with a valid token, read the local public timeline in chunks of 40, and iterate until 500 usable rows are collected. Enforce English only and skip sensitive posts. Strip HTML to keep an original_text copy, and produce a fully normalized content_text by replacing URLs with a [URL] marker, applying NFKC Unicode normalization, and collapsing whitespace. Reject very short posts under 20 characters. Capture id, created_at, account_acct, content_text, and original_text for each accepted post. Respect rate limits by waiting automatically and paginate with fetch_next. When done, write a headered CSV and report how many rows were loaded or saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2722c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 data rows in posts.csv\n",
      "Loaded 500 rows from posts.csv\n",
      "Total rows: 500\n"
     ]
    }
   ],
   "source": [
    "from mastodon import Mastodon\n",
    "from pathlib import Path\n",
    "\n",
    "import csv, re, html, time, unicodedata\n",
    "\n",
    "POSTS_CSV = \"posts.csv\"\n",
    "API_URL = \"https://mastodon.social\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "def have_data():\n",
    "    p = Path(POSTS_CSV)\n",
    "\n",
    "    if not p.exists() or p.stat().st_size == 0:\n",
    "        return False\n",
    "\n",
    "    with p.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        line_count = sum(1 for _ in f)\n",
    "\n",
    "    print(f\"Found {line_count - 1} data rows in {POSTS_CSV}\")\n",
    "    return max(0, line_count) > 500\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    with open(POSTS_CSV, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            rows.append(row)\n",
    "\n",
    "    print(f\"Loaded {len(rows)} rows from {POSTS_CSV}\")\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    mastodon = Mastodon(\n",
    "        access_token=ACCESS_TOKEN,\n",
    "        api_base_url=API_URL,\n",
    "        ratelimit_method=\"wait\",\n",
    "    )\n",
    "\n",
    "    posts = mastodon.timeline_public(limit=40, local=True)\n",
    "\n",
    "    while posts and len(rows) < 500:\n",
    "        for post in posts:\n",
    "            # only use English posts\n",
    "            if not post.get(\"language\") == \"en\":\n",
    "                continue\n",
    "\n",
    "            # skip sensitive posts\n",
    "            if post.get(\"sensitive\", False):\n",
    "                continue\n",
    "\n",
    "            content = post.get(\"content\", \"\")\n",
    "\n",
    "            # original text (only strip HTML tags)\n",
    "            original_text = strip_html(content)\n",
    "\n",
    "            # clean and normalize text\n",
    "            content_text = clean_text(content)\n",
    "\n",
    "            # skip short posts\n",
    "            if len(content_text) < 20:\n",
    "                continue\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"id\": post[\"id\"],\n",
    "                    \"created_at\": post[\"created_at\"].isoformat(),\n",
    "                    \"account_acct\": post[\"account\"][\"acct\"],\n",
    "                    \"content_text\": content_text,\n",
    "                    \"original_text\": original_text,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if len(rows) >= 500:\n",
    "                break\n",
    "\n",
    "        if len(rows) >= 500:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.1)\n",
    "        posts = mastodon.fetch_next(posts)\n",
    "\n",
    "\n",
    "def save_data():\n",
    "    fieldnames = list(rows[0].keys()) if rows else []\n",
    "\n",
    "    with open(POSTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "    print(f\"Wrote {len(rows)} rows to {POSTS_CSV}\")\n",
    "\n",
    "\n",
    "def strip_html(s):\n",
    "    # strip HTML tags\n",
    "    s = re.sub(r\"<br\\s*/?>\", \" \", s, flags=re.I)\n",
    "    s = re.sub(r\"</p\\s*>\", \" \", s, flags=re.I)\n",
    "    s = re.sub(r\"<.*?>\", \"\", s)\n",
    "    s = html.unescape(s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def clean_text(s):\n",
    "    s = strip_html(s)\n",
    "\n",
    "    # replace URLs with a marker to keep signal\n",
    "    s = re.sub(\n",
    "        r\"(https?://\\S+|www\\.\\S+|\\b[\\w-]+(?:\\.[\\w-]+)+(?:/\\S*)?)\",\n",
    "        \" [URL] \",\n",
    "        s,\n",
    "        flags=re.I,\n",
    "    )\n",
    "\n",
    "    # Unicode normalize\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "    # collapse whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # trim leading/trailing whitespace\n",
    "    s = s.strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "if have_data():\n",
    "    load_data()\n",
    "else:\n",
    "    get_data()\n",
    "    save_data()\n",
    "\n",
    "print(f\"Total rows: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f1e33",
   "metadata": {},
   "source": [
    "#### Perform sentence split and tokenization\n",
    "\n",
    "Load a spaCy English pipeline with sentence segmentation enabled. Read each row's content_text and pair it with the full row so metadata is attached. Stream pairs through nlp.pipe to segment sentences and tokenize without removing punctuation or casing. For each document, build a record that preserves the original text, the sentence splits, the surface tokens, the lemmas with hashtags, mentions, and the URL marker kept as-is, and the coarse part-of-speech tags.\n",
    "\n",
    "Compute simple sentiment features directly from the document. Count exclamation marks and question marks. Count URL markers, hashtags, and mentions. Count all caps tokens while ignoring the URL marker. Detect elongated tokens and a emoji presence using code point checks. Write one JSON object per line to preprocessed_posts.jsonl so downstream steps can score without reprocessing. Report processed record counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b36828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 records processed and saved to preprocessed_posts.jsonl\n"
     ]
    }
   ],
   "source": [
    "import spacy, json\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "\n",
    "PREPROCESSED_FILE = \"preprocessed_posts.jsonl\"\n",
    "\n",
    "\n",
    "def sentiment_features(doc):\n",
    "    txt = doc.text\n",
    "    return {\n",
    "        \"exclamations\": txt.count(\"!\"),\n",
    "        \"questions\": txt.count(\"?\"),\n",
    "        \"all_caps_tokens\": sum(\n",
    "            1 for t in doc if t.is_alpha and t.text.isupper() and len(t) > 1\n",
    "        ),\n",
    "        \"elongated_tokens\": sum(\n",
    "            1 for t in doc if __import__(\"re\").search(r\"(.)\\1{2,}\", t.text)\n",
    "        ),\n",
    "        \"emoji_tokens\": sum(1 for t in doc if any(ord(ch) >= 0x1F300 for ch in t.text)),\n",
    "        \"features\": {\n",
    "            \"exclamations\": txt.count(\"!\"),\n",
    "            \"questions\": txt.count(\"?\"),\n",
    "            \"url_count\": sum(1 for t in doc if t.text == \"URL\"),\n",
    "            \"hashtag_count\": sum(1 for t in doc if t.text.startswith(\"#\")),\n",
    "            \"mention_count\": sum(1 for t in doc if t.text.startswith(\"@\")),\n",
    "            \"all_caps_tokens\": sum(\n",
    "                1 for t in doc\n",
    "                if t.is_alpha and t.text.isupper() and len(t) > 1 and t.text != \"URL\"\n",
    "            ),\n",
    "            \"elongated_tokens\": sum(1 for t in doc if __import__(\"re\").search(r\"(.)\\1{2,}\", t.text)),\n",
    "            \"emoji_tokens\": sum(1 for t in doc if any(ord(ch) >= 0x1F300 for ch in t.text)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "for r in rows:\n",
    "    t = r.get(\"content_text\")\n",
    "    \n",
    "    if isinstance(t, str) and t.strip():\n",
    "        records.append((t, r))\n",
    "\n",
    "with open(PREPROCESSED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc, meta in nlp.pipe(records, as_tuples=True, batch_size=64):\n",
    "        toks = [t for t in doc if not t.is_space]\n",
    "\n",
    "        rec = {\n",
    "            \"id\": meta.get(\"id\"),\n",
    "            \"text\": doc.text,\n",
    "            \"sentences\": [[t.text for t in s] for s in doc.sents],\n",
    "            \"tokens\": [t.text for t in toks],\n",
    "            \"lemmas\": [\n",
    "                t.text if t.text.startswith(\"#\") or t.text.startswith(\"@\") or t.text == \"URL\"\n",
    "                else t.lemma_\n",
    "                for t in toks\n",
    "            ],\n",
    "            \"pos\": [t.pos_ for t in toks],\n",
    "            \"features\": sentiment_features(doc),\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"{len(records)} records processed and saved to {PREPROCESSED_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadf1ec",
   "metadata": {},
   "source": [
    "#### Label posts sentiments and scores\n",
    "\n",
    "Load a pretrained sentiment classifier and run it over the preprocessed posts. Read each JSONL record, classify the text with the Twitter RoBERTa pipeline using truncation to 256 tokens. Collect the lowercase label and confidence score along with the original id and text. Write the accumulated predictions to a CSV. Print the predictions count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0543c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote 500 predictions to sentiment_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json, csv\n",
    "\n",
    "PREDICTIONS_FILE = \"sentiment_predictions.csv\"\n",
    "\n",
    "clf = pipeline(\n",
    "    \"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "preds = []\n",
    "\n",
    "with open(PREPROCESSED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        out = clf(rec[\"text\"], truncation=True, max_length=256)[0]\n",
    "\n",
    "        preds.append(\n",
    "            {\n",
    "                \"id\": rec[\"id\"],\n",
    "                \"label\": out[\"label\"].lower(),\n",
    "                \"score\": float(out[\"score\"]),\n",
    "                \"text\": rec[\"text\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "with open(PREDICTIONS_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\", \"label\", \"score\", \"text\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(preds)\n",
    "\n",
    "print(f\"\\nWrote {len(preds)} predictions to {PREDICTIONS_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
